{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vision_transfomer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIO8fYVCE4pPz/pZcntj/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04a95a5871fe456bab1d215099a3af0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f5603c1cbdf4d938c5a528680ebec9c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff5cdd9a7acd4b3b8070dd3dc6fc6274",
              "IPY_MODEL_69280c9d30994dc9936de1e8fdc7f90a"
            ]
          }
        },
        "0f5603c1cbdf4d938c5a528680ebec9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff5cdd9a7acd4b3b8070dd3dc6fc6274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_449ddb93caa24a55bbe84a9c1486cd62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fcdaaf503ca425f97d66dab3db8fdc7"
          }
        },
        "69280c9d30994dc9936de1e8fdc7f90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14b209e127d14cc69681da17ca81f062",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:07&lt;00:00, 22357932.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e14a3eebd9e42c4837ed24888e78643"
          }
        },
        "449ddb93caa24a55bbe84a9c1486cd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fcdaaf503ca425f97d66dab3db8fdc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14b209e127d14cc69681da17ca81f062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e14a3eebd9e42c4837ed24888e78643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kznfrd/ml_study/blob/main/vision_transfomer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3TcabRzUxYP"
      },
      "source": [
        "### vision transformerを試してみる\n",
        "https://recruit.gmo.jp/engineer/jisedai/blog/vision_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "04a95a5871fe456bab1d215099a3af0f",
            "0f5603c1cbdf4d938c5a528680ebec9c",
            "ff5cdd9a7acd4b3b8070dd3dc6fc6274",
            "69280c9d30994dc9936de1e8fdc7f90a",
            "449ddb93caa24a55bbe84a9c1486cd62",
            "5fcdaaf503ca425f97d66dab3db8fdc7",
            "14b209e127d14cc69681da17ca81f062",
            "5e14a3eebd9e42c4837ed24888e78643"
          ]
        },
        "id": "G53ZvE0vUuZd",
        "outputId": "0c82ffd0-14b7-46a9-af64-f9de9089063c"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "classes = ('plane', 'car', 'bird', 'cat', \n",
        "'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04a95a5871fe456bab1d215099a3af0f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "107KFOYIVZk6",
        "outputId": "1ce9c8ab-0510-4639-ded8-efefb84c8763"
      },
      "source": [
        "pip install vit-pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/31/40/56919a1be6b596f30a692d4855f2d7bde8945e95cd4eb1c6588c109d4581/vit_pytorch-0.6.7-py3-none-any.whl\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from vit-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->vit-pytorch) (0.8)\n",
            "Installing collected packages: einops, vit-pytorch\n",
            "Successfully installed einops-0.3.0 vit-pytorch-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5NsGK7RU-DR"
      },
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net = ViT(\n",
        "    image_size=32,\n",
        "    patch_size=4,\n",
        "    num_classes=10,\n",
        "    dim=256,\n",
        "    depth=3,\n",
        "    heads=4,\n",
        "    mlp_dim=256,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ").to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J73s9l2aVkwu",
        "outputId": "76750927-4a9c-4037-df4b-aabc338b2cb7"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "epochs = 20\n",
        "for epoch in range(0, epochs):\n",
        "    epoch_train_loss = 0\n",
        "    epoch_train_acc = 0\n",
        "    epoch_test_loss = 0\n",
        "    epoch_test_acc = 0\n",
        "    net.train()\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()/len(train_loader)\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "        epoch_train_acc += acc/len(train_loader)\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_test_loss += loss.item()/len(test_loader)\n",
        "            test_acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "            epoch_test_acc += test_acc/len(test_loader)\n",
        "    print(f'Epoch {epoch+1} : train acc. {epoch_train_acc:.2f} train loss {epoch_train_loss:.2f}')\n",
        "    print(f'Epoch {epoch+1} : test acc. {epoch_test_acc:.2f} test loss {epoch_test_loss:.2f}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : train acc. 0.17 train loss 2.21\n",
            "Epoch 1 : test acc. 0.22 test loss 2.09\n",
            "Epoch 2 : train acc. 0.22 train loss 2.07\n",
            "Epoch 2 : test acc. 0.28 test loss 1.96\n",
            "Epoch 3 : train acc. 0.29 train loss 1.94\n",
            "Epoch 3 : test acc. 0.33 test loss 1.88\n",
            "Epoch 4 : train acc. 0.32 train loss 1.88\n",
            "Epoch 4 : test acc. 0.34 test loss 1.83\n",
            "Epoch 5 : train acc. 0.34 train loss 1.82\n",
            "Epoch 5 : test acc. 0.36 test loss 1.77\n",
            "Epoch 6 : train acc. 0.36 train loss 1.78\n",
            "Epoch 6 : test acc. 0.38 test loss 1.73\n",
            "Epoch 7 : train acc. 0.37 train loss 1.75\n",
            "Epoch 7 : test acc. 0.39 test loss 1.70\n",
            "Epoch 8 : train acc. 0.39 train loss 1.71\n",
            "Epoch 8 : test acc. 0.41 test loss 1.66\n",
            "Epoch 9 : train acc. 0.40 train loss 1.67\n",
            "Epoch 9 : test acc. 0.42 test loss 1.63\n",
            "Epoch 10 : train acc. 0.41 train loss 1.64\n",
            "Epoch 10 : test acc. 0.44 test loss 1.59\n",
            "Epoch 11 : train acc. 0.42 train loss 1.61\n",
            "Epoch 11 : test acc. 0.44 test loss 1.56\n",
            "Epoch 12 : train acc. 0.43 train loss 1.58\n",
            "Epoch 12 : test acc. 0.46 test loss 1.53\n",
            "Epoch 13 : train acc. 0.44 train loss 1.56\n",
            "Epoch 13 : test acc. 0.46 test loss 1.51\n",
            "Epoch 14 : train acc. 0.45 train loss 1.53\n",
            "Epoch 14 : test acc. 0.46 test loss 1.53\n",
            "Epoch 15 : train acc. 0.46 train loss 1.51\n",
            "Epoch 15 : test acc. 0.49 test loss 1.46\n",
            "Epoch 16 : train acc. 0.47 train loss 1.49\n",
            "Epoch 16 : test acc. 0.48 test loss 1.45\n",
            "Epoch 17 : train acc. 0.47 train loss 1.47\n",
            "Epoch 17 : test acc. 0.49 test loss 1.44\n",
            "Epoch 18 : train acc. 0.48 train loss 1.44\n",
            "Epoch 18 : test acc. 0.50 test loss 1.42\n",
            "Epoch 19 : train acc. 0.49 train loss 1.42\n",
            "Epoch 19 : test acc. 0.50 test loss 1.43\n",
            "Epoch 20 : train acc. 0.50 train loss 1.40\n",
            "Epoch 20 : test acc. 0.51 test loss 1.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2TLW5F_ZKZU"
      },
      "source": [
        "```\n",
        "さて、ここまで進めておいてネタバラシという理由ではないですが、実は ViT の学習には注意が必要です。ViTでは、まずJFT-300Mという3億枚のデータセット(非公開のようです)を元に事前学習し、その後に各種タスク別に fine-tuning しており、論文によると、膨大なデータセットでの事前学習が個別のタスクへの高い性能を発揮すために必要だと書かれています。事前学習済みのデータが公開されていますので(https://github.com/google-research/vision_transformer)、これを元に個別のタスク向けにチューニングするのが良いのだと思います(JFT-300Mで事前学習したものは未公開のようです)。ネットでViTを検索すると精度が悪いという記事が見つかりますが、これが原因かと思われます。\n",
        "```"
      ]
    }
  ]
}